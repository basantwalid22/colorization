{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "<torch.cuda.device object at 0x0000015705D61F40>\n",
      "1\n",
      "NVIDIA GeForce MX350\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  print(torch.cuda.current_device())\n",
    "  print(torch.cuda.device(0))\n",
    "  print(torch.cuda.device_count())\n",
    "  print(torch.cuda.get_device_name(0))\n",
    "else:\n",
    "  print(\"No NVIDIA driver found. Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColorizationNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=5, stride=1, padding=4, dilation=2)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=4, dilation=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=4, dilation=2)\n",
    "        self.conv4 = nn.Conv2d(128, 3, kernel_size=5, stride=1, padding=4, dilation=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = nn.functional.relu(self.conv3(x))\n",
    "        x = torch.sigmoid(self.conv4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ColorizationNet().to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def rgb_to_gray(img):\n",
    "    return img.mean(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [1/782], Loss: 0.0047\n",
      "Epoch [1/5], Step [301/782], Loss: 0.0056\n",
      "Epoch [1/5], Step [601/782], Loss: 0.0046\n",
      "Epoch [2/5], Step [1/782], Loss: 0.0048\n",
      "Epoch [2/5], Step [301/782], Loss: 0.0047\n",
      "Epoch [2/5], Step [601/782], Loss: 0.0047\n",
      "Epoch [3/5], Step [1/782], Loss: 0.0077\n",
      "Epoch [3/5], Step [301/782], Loss: 0.0056\n",
      "Epoch [3/5], Step [601/782], Loss: 0.0053\n",
      "Epoch [4/5], Step [1/782], Loss: 0.0043\n",
      "Epoch [4/5], Step [301/782], Loss: 0.0057\n",
      "Epoch [4/5], Step [601/782], Loss: 0.0058\n",
      "Epoch [5/5], Step [1/782], Loss: 0.0044\n",
      "Epoch [5/5], Step [301/782], Loss: 0.0059\n",
      "Epoch [5/5], Step [601/782], Loss: 0.0047\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "training_losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    for i, (images, _) in enumerate(train_loader):\n",
    "        grayscale_images = rgb_to_gray(images).to(device)\n",
    "        images = images.to(device)\n",
    "\n",
    "        outputs = model(grayscale_images)\n",
    "        loss = criterion(outputs, images)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        training_losses.append(loss.item())\n",
    "\n",
    "        if i % 300 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{EPOCHS}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# حفظ النموذج بعد الانتهاء من التدريب\n",
    "torch.save(model.state_dict(), 'final_model.pth')  # حفظ النموذج بعد التدريب بالكامل\n",
    "\n",
    "print(\"Finished Training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-27 17:34:52.247 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-27 17:34:52.666 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\mohammed\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-12-27 17:34:52.667 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-27 17:34:52.668 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-27 17:34:52.668 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-27 17:34:52.668 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-27 17:34:52.669 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-27 17:34:52.670 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-27 17:34:52.670 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-27 17:34:52.671 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-27 17:34:52.671 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-27 17:34:52.672 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-27 17:34:52.673 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from moviepy.editor import VideoFileClip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "\n",
    "# Set up the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the pre-trained model\n",
    "class ColorizationNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColorizationNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=5, stride=1, padding=4, dilation=2)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=4, dilation=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=4, dilation=2)\n",
    "        self.conv4 = nn.Conv2d(128, 3, kernel_size=5, stride=1, padding=4, dilation=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = nn.functional.relu(self.conv3(x))\n",
    "        x = torch.sigmoid(self.conv4(x))\n",
    "        return x\n",
    "\n",
    "model = ColorizationNet().to(device)\n",
    "model.load_state_dict(torch.load('final_model.pth'))  # Assuming you've already trained and saved your model\n",
    "\n",
    "# Define transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Streamlit UI\n",
    "st.title(\"AI Image and Video Colorizer\")\n",
    "\n",
    "# Image Upload\n",
    "uploaded_image = st.file_uploader(\"Upload an Image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
    "if uploaded_image:\n",
    "    img = Image.open(uploaded_image)\n",
    "    st.image(img, caption=\"Uploaded Image\", use_column_width=True)\n",
    "\n",
    "    # Transform and colorize the image\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        colorized_tensor = model(img_tensor)\n",
    "    \n",
    "    colorized_img = transforms.ToPILImage()(colorized_tensor.squeeze(0).cpu())\n",
    "    st.image(colorized_img, caption=\"Colorized Image\", use_column_width=True)\n",
    "\n",
    "# Video Upload\n",
    "uploaded_video = st.file_uploader(\"Upload a Video\", type=[\"mp4\", \"mov\", \"avi\"])\n",
    "if uploaded_video:\n",
    "    clip = VideoFileClip(uploaded_video)\n",
    "    st.video(uploaded_video)\n",
    "\n",
    "    # Colorize video frames\n",
    "    colorized_frames = []\n",
    "    for frame in clip.iter_frames():\n",
    "        frame_gray_tensor = transform(Image.fromarray(frame)).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            frame_colorized = model(frame_gray_tensor)\n",
    "        colorized_frames.append(frame_colorized.cpu())\n",
    "    \n",
    "    # Save colorized video\n",
    "    output_video_path = \"colorized_output.mp4\"\n",
    "    colorized_clip = VideoFileClip(uploaded_video)\n",
    "    colorized_clip = colorized_clip.set_fps(clip.fps)\n",
    "\n",
    "    def colorize_frame(t):\n",
    "        index = min(int(t * clip.fps), len(colorized_frames) - 1)\n",
    "        frame_colorized = colorized_frames[index].squeeze(0).permute(1, 2, 0).numpy()\n",
    "        return (255 * frame_colorized).astype('uint8')\n",
    "\n",
    "    colorized_clip = colorized_clip.fl(lambda gf, t: colorize_frame(t), apply_to=['mask', 'audio'])\n",
    "    colorized_clip.write_videofile(output_video_path, codec='libx264')\n",
    "\n",
    "    # Provide the colorized video download link\n",
    "    st.download_button(\"Download Colorized Video\", data=open(output_video_path, \"rb\"), file_name=\"colorized_video.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
